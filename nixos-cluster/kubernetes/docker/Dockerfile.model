# Build a per-model, per-GPU image on top of the Ollama base
# The build pre-pulls the model to avoid cold-starts

ARG BASE_IMAGE=ollama/ollama:latest
FROM ${BASE_IMAGE}

ARG MODEL
ARG GPU_ARCH

LABEL org.opencontainers.image.title="ollama-${MODEL}-${GPU_ARCH}" \
      model="${MODEL}" \
      gpu="${GPU_ARCH}"

ENV OLLAMA_MODELS=/models \
    OLLAMA_HOST=0.0.0.0

RUN mkdir -p /models

# Optionally, set CUDA/BLAS toggles here if needed for specific GPUs
# ENV NVIDIA_VISIBLE_DEVICES=all
# ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Pre-warm the image with the requested model (POSIX sh)
# Start a background server, give it a moment, pull the model, then stop it
RUN (ollama serve & echo $! > /tmp/ollama.pid); \
    sleep 120; \
    ollama pull ${MODEL} || (sleep 10; ollama pull ${MODEL}); \
    kill $(cat /tmp/ollama.pid) || true; \
    rm -f /tmp/ollama.pid

EXPOSE 11434

ENTRYPOINT ["ollama", "serve"]

